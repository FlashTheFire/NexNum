# =============================================================================
# NEXNUM ALERTING RULES
# =============================================================================
# 
# These rules trigger alerts sent to Alertmanager, which routes them to
# configured receivers (Telegram, Slack, Email, etc.)
#
# Alert Severity Levels:
#   - critical: Requires immediate attention (pages on-call)
#   - warning: Should be addressed soon (notifications)
#   - info: Informational, may not require action
#
# =============================================================================

groups:
  # ===========================================================================
  # API & APPLICATION HEALTH
  # ===========================================================================
  - name: api_alerts
    interval: 15s
    rules:
      # -----------------------------------------------------------------------
      # HIGH ERROR RATE
      # -----------------------------------------------------------------------
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status_code=~"5.."}[5m])) /
            sum(rate(http_requests_total[5m]))
          ) * 100 > 5
        for: 2m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "ðŸš¨ High API error rate ({{ printf \"%.1f\" $value }}%)"
          description: "Error rate has exceeded 5% for the last 2 minutes. Check logs immediately."
          runbook_url: "https://wiki.nexnum.com/runbooks/high-error-rate"

      - alert: ElevatedErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status_code=~"5.."}[5m])) /
            sum(rate(http_requests_total[5m]))
          ) * 100 > 1
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "âš ï¸ Elevated error rate ({{ printf \"%.1f\" $value }}%)"
          description: "Error rate above 1% for 5 minutes. Monitor closely."

      # -----------------------------------------------------------------------
      # SLOW API RESPONSES
      # -----------------------------------------------------------------------
      - alert: SlowAPIResponses
        expr: |
          histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 3m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "ðŸ¢ Slow API responses (P95: {{ printf \"%.2f\" $value }}s)"
          description: "95th percentile latency exceeds 2 seconds."

      - alert: CriticalAPILatency
        expr: |
          histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 5
        for: 2m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "ðŸš¨ Critical API latency (P99: {{ printf \"%.2f\" $value }}s)"
          description: "99th percentile latency exceeds 5 seconds. Check DB/Redis."

      # -----------------------------------------------------------------------
      # API DOWN
      # -----------------------------------------------------------------------
      - alert: APIDown
        expr: up{job="nexnum-api"} == 0
        for: 1m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "ðŸ”´ API is DOWN"
          description: "NexNum API is not responding to health checks."

  # ===========================================================================
  # PURCHASE & WALLET
  # ===========================================================================
  - name: business_alerts
    interval: 30s
    rules:
      # -----------------------------------------------------------------------
      # PURCHASE FAILURES
      # -----------------------------------------------------------------------
      - alert: HighPurchaseFailureRate
        expr: |
          (
            sum(rate(wallet_transactions_total{type="purchase",status="error"}[10m])) /
            sum(rate(wallet_transactions_total{type="purchase"}[10m]))
          ) * 100 > 10
        for: 5m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "ðŸ’³ High purchase failure rate ({{ printf \"%.1f\" $value }}%)"
          description: "More than 10% of purchases are failing. Check provider status."

      # -----------------------------------------------------------------------
      # PROVIDER FAILURES
      # -----------------------------------------------------------------------
      - alert: ProviderHighErrorRate
        expr: |
          (
            sum by(provider) (rate(provider_api_calls_total{status="error"}[10m])) /
            sum by(provider) (rate(provider_api_calls_total[10m]))
          ) * 100 > 20
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "ðŸ“¡ Provider {{ $labels.provider }} error rate high ({{ printf \"%.1f\" $value }}%)"
          description: "Consider disabling or switching provider."

      - alert: ProviderDown
        expr: |
          sum by(provider) (rate(provider_api_calls_total{status="error"}[5m])) > 0 and
          sum by(provider) (rate(provider_api_calls_total{status="success"}[5m])) == 0
        for: 3m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "ðŸ”´ Provider {{ $labels.provider }} appears DOWN"
          description: "No successful calls in last 5 minutes, only errors."

  # ===========================================================================
  # QUEUE & WORKERS
  # ===========================================================================
  - name: queue_alerts
    interval: 30s
    rules:
      # -----------------------------------------------------------------------
      # QUEUE BACKLOG
      # -----------------------------------------------------------------------
      - alert: QueueBacklogHigh
        expr: queue_depth{state="active"} > 100
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "ðŸ“¥ Queue backlog high ({{ $value }} jobs)"
          description: "Active queue has more than 100 pending jobs."

      - alert: QueueBacklogCritical
        expr: queue_depth{state="active"} > 500
        for: 3m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "ðŸš¨ Queue backlog CRITICAL ({{ $value }} jobs)"
          description: "Queue is severely backed up. Scale workers or investigate."

      # -----------------------------------------------------------------------
      # FAILED JOBS
      # -----------------------------------------------------------------------
      - alert: HighFailedJobRate
        expr: |
          increase(queue_depth{state="failed"}[30m]) > 10
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "âŒ High job failure rate ({{ $value }} failures in 30m)"
          description: "Jobs are failing at elevated rate. Check worker logs."

      # -----------------------------------------------------------------------
      # STUCK QUEUE
      # -----------------------------------------------------------------------
      - alert: QueueStuck
        expr: |
          queue_depth{state="active"} > 0 and 
          rate(queue_depth{state="active"}[10m]) == 0
        for: 10m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "ðŸ”’ Queue appears STUCK"
          description: "Active jobs exist but no progress for 10 minutes. Workers may be dead."

  # ===========================================================================
  # INFRASTRUCTURE
  # ===========================================================================
  - name: infra_alerts
    interval: 60s
    rules:
      # -----------------------------------------------------------------------
      # MEMORY
      # -----------------------------------------------------------------------
      - alert: HighMemoryUsage
        expr: nodejs_heap_size_used_bytes / nodejs_heap_size_total_bytes * 100 > 85
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "ðŸ’¾ High memory usage ({{ printf \"%.0f\" $value }}%)"
          description: "Node.js heap usage exceeds 85%."

      - alert: CriticalMemoryUsage
        expr: nodejs_heap_size_used_bytes / nodejs_heap_size_total_bytes * 100 > 95
        for: 2m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "ðŸš¨ Critical memory usage ({{ printf \"%.0f\" $value }}%)"
          description: "OOM likely imminent. Restart or scale."

      # -----------------------------------------------------------------------
      # DATABASE
      # -----------------------------------------------------------------------
      - alert: DatabaseConnectionsHigh
        expr: pg_stat_activity_count > 90
        for: 5m
        labels:
          severity: warning
          team: dba
        annotations:
          summary: "ðŸ—„ï¸ Database connections high ({{ $value }})"
          description: "Approaching connection limit."

      # -----------------------------------------------------------------------
      # REDIS  
      # -----------------------------------------------------------------------
      - alert: RedisDown
        expr: up{job="redis"} == 0
        for: 1m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "ðŸ”´ Redis is DOWN"
          description: "Redis not responding. Sessions and caching affected."

  # ===========================================================================
  # AUTHENTICATION & SECURITY
  # ===========================================================================
  - name: security_alerts
    interval: 60s
    rules:
      # -----------------------------------------------------------------------
      # FAILED LOGIN SPIKE
      # -----------------------------------------------------------------------
      - alert: FailedLoginSpike
        expr: |
          increase(auth_events_total{event="login",status="failure"}[5m]) > 50
        for: 2m
        labels:
          severity: warning
          team: security
        annotations:
          summary: "ðŸ” Failed login spike ({{ $value }} in 5m)"
          description: "Possible brute force attack. Review IPs."

      # -----------------------------------------------------------------------
      # REGISTRATION SPIKE
      # -----------------------------------------------------------------------
      - alert: RegistrationSpike
        expr: |
          increase(auth_events_total{event="register",status="success"}[10m]) > 100
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "ðŸ“ˆ Registration spike ({{ $value }} in 10m)"
          description: "Unusual signup activity. May be bot attack."
